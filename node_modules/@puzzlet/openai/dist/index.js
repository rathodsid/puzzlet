// src/openai-chat.ts
import { createOpenAI } from "@ai-sdk/openai";
var OpenAIChatPlugin = class {
  constructor() {
    this.apiKey = "";
    this.provider = "openai";
  }
  setApiKey(apiKey) {
    this.apiKey = apiKey;
  }
  serialize(completionParams, name, api) {
    const { model, messages, tools, stream_options, tool_choice, ...settings } = completionParams;
    const frontMatterData = {
      name,
      metadata: {
        model: {
          name: model,
          settings: settings || {}
        }
      }
    };
    if (tools) {
      const transformedTools = tools.reduce((acc, { function: func }) => {
        acc[func.name] = {
          description: func.description,
          parameters: func.parameters
        };
        return acc;
      }, {});
      if (tool_choice === "auto") {
        frontMatterData.metadata.model.settings.tools = transformedTools;
      } else {
        const schemaTool = tools.find((tool) => tool.function.parameters);
        if (schemaTool) {
          frontMatterData.metadata.model.settings.schema = schemaTool.function.parameters;
        }
      }
    }
    const frontMatter = api.toFrontMatter(frontMatterData);
    const messageBody = messages.map((message) => {
      const role = message.role;
      const JSXTag = role.charAt(0).toUpperCase() + role.slice(1);
      return `<${JSXTag}>${message.content}</${JSXTag}>`;
    }).join("\n");
    return `${frontMatter}
${messageBody}`;
  }
  async deserialize(agentMark, api, config) {
    const { metadata, messages } = agentMark;
    const { model: modelConfig } = metadata;
    const completionParamsPromise = new Promise(
      async (resolve) => {
        const openai = createOpenAI({
          compatibility: "strict",
          fetch: async (_, options) => {
            const requestBody = JSON.parse(options.body);
            resolve(requestBody);
            return new Response();
          }
        });
        const providerModel = openai(modelConfig.name);
        try {
          if (config == null ? void 0 : config.withStream) {
            await api.streamInference(modelConfig.settings, providerModel, messages);
          } else {
            await api.runInference(modelConfig.settings, providerModel, messages);
          }
        } catch (e) {
        }
      }
    );
    return completionParamsPromise;
  }
  async runInference(agentMark, api, options) {
    const apiKey = (options == null ? void 0 : options.apiKey) || this.apiKey || api.getEnv("OPENAI_API_KEY");
    if (!apiKey) {
      throw new Error("No API key provided");
    }
    const openai = createOpenAI({
      compatibility: "strict",
      apiKey,
      fetch: api.fetch
    });
    const { metadata, messages } = agentMark;
    const { model: modelConfig } = metadata;
    const providerModel = openai(modelConfig.name);
    const result = await api.runInference(modelConfig.settings, providerModel, messages, options);
    return result;
  }
  async streamInference(agentMark, api, options) {
    const apiKey = (options == null ? void 0 : options.apiKey) || this.apiKey || api.getEnv("OPENAI_API_KEY");
    if (!apiKey) {
      throw new Error("No API key provided");
    }
    const openai = createOpenAI({
      compatibility: "strict",
      apiKey,
      fetch: api.fetch
    });
    const { metadata, messages } = agentMark;
    const { model: modelConfig } = metadata;
    const providerModel = openai(modelConfig.name);
    const result = await api.streamInference(modelConfig.settings, providerModel, messages, options);
    return result;
  }
};

// src/index.ts
var src_default = OpenAIChatPlugin;
export {
  OpenAIChatPlugin,
  src_default as default
};
//# sourceMappingURL=index.js.map